\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url,fancybox,wrapfig}
\usepackage{multirow,tabularx}
\usepackage[brazil]{babel}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath}

\sloppy

\title{Sistema que Recomenda Sistema:\\
       uma Solução para Recomendação\\
       de Componentes de Software}

\author{Tássia Camões Araújo \inst{1} }

\address{Instituto de Matemática e Estatística \\
Universidade de São Paulo (USP)}

\begin{document}

\maketitle
\vspace{1mm}
\begin{center}
EXAME DE QUALIFICAÇÃO DE MESTRADO\\
Programa: Ciência da Computação\\
Orientador: Prof. Dr. Arnaldo Mandel
\end{center}

%\begin{abstract}
%\end{abstract}

\begin{resumo}
  A crescente expansão na oferta de serviços na rede mundial de computadores
  vem expondo cada vez mais seus usuários a inúmeras possibilidades de escolha.
  Esta visível pluralidade, somada à infinidade de interesses dos indivíduos
  envolvidos, acaba por demandar uma organização das informações de forma que
  seja possível aproximar destes usuários aquilo que supõe-se ser o que
  verdadeiramente necessitam. Tal suposição é auxiliada por sistemas
  computacionais de recomendação, que em geral utilizam o próprio comportamento
  do usuário como componente fundamental para decidir o que lhe deve ser
  apresentado como opções mais suscetíveis a aceitação. O presente trabalho
  propõe o desenvolvimento de um sistema que auxilia a recomendação de
  componentes de software. Vê-se em especial no universo dos softwares de
  código aberto uma enorme diversidade de opções que, pelo seu caráter
  majoritariamente não comercial, são apresentadas de forma menos ostensiva aos
  seus potenciais usuários. A distribuição Debian GNU/Linux reúne
  milhares de componentes de software nestas circunstâncias sob uma sólida
  infraestrutura, oferendo assim a este trabalho uma série de benefícios
  técnicos para os experimentos. Esta distribuição será portanto utilizada,
  onde propõe-se a implementação e posterior análise comparativa das técnicas
  abordadas de recomendação, que por fim será objeto de uma consulta pública
  aos usuários acerca da sua eficácia, a ser também compilada e comentada na
  finalização do trabalho.
\end{resumo}

\newpage

\section{Introdução} \label{sec:intro}

A popularização de recursos computacionais e do acesso à Internet nas
últimas décadas proporcionou um aumento expressivo na quantidade de serviços e
conteúdo à disposição dos usuários. Um dos fatores para este aumento é que
os usuários, que anteriormente eram considerados meros consumidores,
apresentam-se atualmente como produtores de conteúdo. \cite{Castells:06} analisa
este fenômeno e afirma que a maioria da população acredita que pode influenciar
outras pessoas, atuando no mundo através da sua força de vontade e
utilizando seus próprios meios. Isto pode ser observado no surgimento e
proliferação de serviços criados e mantidos pelos usuários: blogs, enciclopédias
colaborativas, como a Wikipedia\footnote{http://wikipedia.org}, repositórios
para compartilhamento de fotografia e vídeo, como Flickr
\footnote{http://flickr.com} e Youtube\footnote{http://youtube.com}, entre
outros. Considerando a produção em termos de software, observa-se o exemplo das
comunidades de software livre, que propiciam a construção coletiva de uma ampla
gama de softwares de qualidade em constante atualização e evolução,
organizados na forma de um rossio \cite{Simon:08}.

A grande diversidade de opções disponíveis nestes ambientes, apesar de positiva,
representa uma sobrecarga de informações que pode confundir o usuário final.
\cite{Yyengar:10} afirma que ``mais é menos'' (em inglês, \emph{``more is less''}), no
sentindo de que quanto maior for a disponibilidade de escolhas, menor será a
satisfação do usuário. O fato é que comumente o indivíduo possui pouca ou
nenhuma experiência pessoal para realizar escolhas em determinado contexto
\cite{Cazella:10}. Sendo assim, recomendações de outras pessoas são de grande
utilidade, pois reduzem as dúvidas e auxiliam o processo de escolha dentre as
muitas alternativas apresentadas. No entanto, diante do número de usuários e do
volume de conteúdo que usualmente deve ser considerado, recomendações no estilo
``boca a boca'' tornam-se ineficientes, pois exigem a comunicação direta entre
os pares. A tecnologia assume então papel fundamental neste processo
\cite{Shardanand:95}.

Sistemas de recomendação emergiram como uma área de pesquisa independente na
década de 90, tendo como fonte de estratégias para a automatização de
recomendações soluções nas áreas de ciência cognitiva, teoria da aproximação,
recuperação da informação, teorias de predição, administração e marketing
\cite{Adomavicius:05}. O tema ganhou destaque com o crescimento do comércio
eletrônico, onde apresentar o que o usuário tem interesse pode significar
conquistar o cliente. Os Sistemas de Recomendação fazem a associação entre
objetos de interesse e pessoas neles interessadas, filtrando as informações de
forma a apresentar somente aquilo que seja relevante para o usuário. Além da
agilidade para encontrar o que se deseja, tais sistemas possibilitam a
personalização de serviços e conteúdos, que são apresentados de maneiras
distintas para usuários diferentes, a partir da identificação de interesses
pessoais.

Este trabalho se insere no contexto de desenvolvimento de componentes de
software, no qual os usuários são modelados como clientes e os componentes
desenvolvidos como itens pelos quais os usuários têm interesse ou não.
Assume-se que cada usuário tem um sistema operacional instalado e deve escolher
quais aplicativos extras deseja obter para suprir suas necessidades pessoais.
Diante da enorme quantidade de software disponíveis, nas mais diversas áreas de
aplicação, configura-se um cenário onde um sistema de recomendação traria
benefícios imediatos ao usuário, por auxiliá-lo a tomar decisões acerca da
configuração do seu ambiente de trabalho.

O objetivo principal deste trabalho é a experimentação de diferentes técnicas e
estratégias para a construção de sistemas recomendadores no domínio de
componentes de software, utilizando como ambiente de desenvolvimento a
distribuição Debian GNU/Linux. Ao final deste estudo pretende-se integrar a
estratégia que obtiver resultados mais satisfatórios à infraestrutura existente
desta distribuição, aproximando os serviços atualmente disponíveis ao estado da
arte em sistemas de recomendação.

O presente texto está organizado como segue. A seção \ref{sec:recomendacao}
traz uma introdução sobre sistemas de recomendação, seus desafios,
as principais técnicas e estratégias utilizadas em seu desenvolvimento, além de
métodos de avaliação destes sistemas. O ambiente de desenvolvimento deste
estudo, distribuições GNU/Linux, é descrito na seção \ref{sec:distribuicoes},
seguida pela exibição de soluções existentes para a recomendação neste domínio
(seção \ref{sec:rec_distro}. Na seção \ref{sec:metodologia} é apresentada a
metodologia de realização deste trabalho e por fim, a seção
\ref{sec:conclusao}, traz a conclusão preliminar deste projeto, até
a presente etapa de execução.


\section{Sistemas de recomendação}\label{sec:recomendacao}

Os Sistemas de Recomendação aumentam a capacidade e eficácia de um processo de
indicação que é bastante popular nas relações sociais \cite{Resnick:97}.
Tradicionalmente as recomendações eram produzidas exclusivamente por
especialistas na área em que se pretendia recomendar. A recomendação de filmes
em cartaz é um exemplo deste tipo de indicação, publicada por críticos de arte
nos principais jornais e revistas do país. Nos últimos anos, porém, a opinião e
o comportamento de usuários não especializados passaram a ser considerados nas
recomendações, por agregarem valor às mesmas. Isto pode acontecer de forma
explícita, quando o próprio usuário escreve sua opinião ou avalia a qualidade
de um item, ou implícita, quando suas preferências, comportamentos e transações
são analisados e incorporados à recomendação, de forma transparente aos usuários.

O problema da recomendação é comumente formalizado através de uma estrutura de
pontuação como representação computacional da utilidade dos itens para os
usuários ou clientes. A partir de avaliações feitas pelos próprios usuários do
sistema, tenta-se estimar pontuações para os itens que ainda não foram
avaliados pelos mesmos. Uma vez que esta estimativa tenha sido feita, pode-se
recomendar os itens com maior pontuação estimada.

O conceito de utilidade, porém, é subjetivo e arduamente mensurável devido às
dificuldades em distinguir qualitativamente e definir quantitativamente os
fatores que a determinam. Portanto, com a ressalva de que estas medidas não
representam necessariamente a realidade, as pontuações são usadas como
aproximações, pois têm como base as avaliações feitas pelos próprios usuários.

%Segundo \cite{Adomavicius:05}, o problema da recomendação pode ser formalizado
%da seguinte maneira: Seja $C$ o conjunto de todos os usuários e $S$ o conjunto
%de todos os itens que podem ser recomendados, como livros, filmes ou
%restaurantes. O espaço $S$ de itens possíveis pode ser muito grande, variando
%de centenas de milhares até milhões de itens em algumas aplicações.
%Similarmente, o espaço de usuários também pode ser muito grande, milhões em
%alguns casos. Seja $u$ uma função que mede a utilidade do item $s$ para o
%usuário $c$, isto é, $u: C x S \rightarrow R$, onde $R$ é um conjunto ordenado
%(por e\-xemplo, inteiros não negativos ou números reais num determinado
%intervalo). Então, para cada usuário $c \in C$, deve-se escolher o item $s' \in
%S$ que maximiza a função de utilidade. Mais formalmente, temos:
%
%\begin{equation}
%\forall c \in C, s'_c = arg_{s \in S} max u(c,s)
%\end{equation}

\subsection{Desafios}

O desenvolvimento de sistemas recomendadores têm como desafios questões
inerentes ao problema de recomendação e sua representação computacional. As
estratégias e técnicas propostas devem levar em conta tais questões e tentar
contorná-las na medidade do possível. Alguns destas questões foram apontadas
por \cite{Vozalis:03} e são citadas a seguir.

\begin{description}
  \item[Qualidade das recomendações.] Usuários esperam recomendações nas
    quais eles possam confiar. Esta confiabilidade é alcançada na medida em
    que se diminui a incidência de falsos positivos, em outras palavras,
    recomendações que não interessam ao usuário;
  \item[Esparsidade.] A existência de poucas relações usuários-item por
    usuário resulta numa matriz de relacionamentos esparsa, o que dificulta
    a localização de usuários com preferências semelhantes (relações de
    vizinhança), resultando em recomendações fracas.
  \item[Escalabilidade.] A complexidade do cálculo de recomendações cresce
    proporcionalmente tanto ao número de clientes quanto à quantidade de itens,
    portanto a escalabilidade dos algoritmos é um ponto importante a ser
    considerado.
  \item[Perda de transitividade de vizinhança.] Usuários que têm comportamento
    semelhante a um determinado usuário $u$ não necessariamente têm
    comportamento semelhante entre si. A captura deste tipo de relação pode ser
    desejável mas em geral esta informação não e resguardada, exigindo a
    aplicação de métodos específicos para tal.
  \item[Sinônimos.] Quando o universo de itens possibilita a existência
    de sinônimos, a solução deve levar esta informação em conta para prover
    melhores resultados.
  \item[Problema da primeira avaliação.] Um item só pode ser recomendado
    se ele tiver sido escolhido por um usuário anteriormente. Portanto, novos
    itens precisam ter um tratamento especial até que sua presença seja
    "notada".
  \item[Problema do usuário incomum.] Indivíduos com opiniões que fogem
    do usual, que não concordam nem discordam consistentemente com nenhum
    grupo, normalmente não se beneficiam de sistemas de recomendações.

\end{description}

[Perfis de usuário e privacidade]

%\subsection{Perfis de Usuário}
%. Identidade\\
%. Geração e manutenção de perfil\\
%. Reputação
%
%\subsection{Privacidade}

\subsection{Técnicas} \label{sec:tecnicas}

O desenvolvimento de sistemas de recomendação tem suas raízes em áreas
distintas e o problema computacional a ser tratado está fortemente
relacionado com outros problemas clássicos, como busca por informação e
classificação.

A fim de obter a informação desejada, o usuário de uma ferramenta de busca
deve traduzir suas necessidades de informação para uma consulta (\emph{query}
em inglês), que normalmente é representada por um conjunto de
\emph{palavras-chave}. A partir dos termos da \emph{query}, o buscador recupera
os documentos da coleção que sejam relevantes para a mesma. Ademais, visto que
a busca pode retornar um número excessivo de documentos, é desejável que este
resultado seja apresentado ao usuário em ordem decrescente de importância,
aumentando assim as chances de a informação desejada ser encontrada com
rapidez. Para tanto, cada documento da coleção deve ter uma pontuação (peso)
que indique sua relevância para a referida \emph{query}. Traçando um paralelo
com o problema de recomendação, a identidade e/ou o comportamento do usuário
representaria a consulta ao sistema de busca, que provocaria o retorno dos
itens de maior peso, ou seja, mais potencialmente relevantes para o usuário.

Na busca por informação, assume-se que as necessidades do usuário são
particulares e passageiras, e por isso a reincidência de \emph{queries} não é
muito frequente \cite{Manning:09}. Porém, em situações onde se observa que as
mesmas consultas são aplicadas repetidamente de tempos em tempos, é
interessante que o sistema dê suporte a consultas permanentes. Desta forma a
computação necessária pode ser realizada previamente, e apresentada sempre que
a consulta for requisitada. Se a classe de documentos que satisfazem a
uma dessas \emph{queries} permanentes é tida como uma categoria, o processo de
realização das consultas prévias pode ser caracterizado como uma classificação.
O problema da classificação diz respeito à determinação de relacionamentos
entre um dado objeto e um conjunto de classes pré-definidas.

O problema de recomendação pode ser visto como uma classificação, na qual os
itens devem ser categorizados entre duas classes: relevantes ou irrelevantes --
os relevantes seriam recomendados. Porém, a definição de consultas ou regras
fixas para uma busca não é uma estratégia eficiente para o problema da
recomendação, porque neste caso a consulta estaria diretamente relacionada com
a identidade do usuário e precisaria ser escrita especialmente para ele.

Todavia, a disciplina de inteligência artificial aborda a questão da
classificação através de estratégias que não se baseiam em busca. Os
classificadores ``inteligentes'' são construídos a partir de um conjunto de
dados tratados como exemplos para treinamento em algoritmos de aprendizado de
máquina.

A seguir são apresentadas algumas técnicas para resolução destes problemas
que também são utilizadas na construção de sistemas de recomendação, dando
suporte às estratégias apresentadas na seção \ref{sec:estrategias}.

%\vspace{-0.1cm}
\subsubsection{Medida tf-idf}

Acrônimo para \emph{term frequency - inverse document frequency}, o
\emph{tf-idf} é uma medida de peso clássica utilizada no desenvolvimento de
buscadores para ordenar os itens por relevância.

A simples presença de um termo da \emph{query} em um documento da coleção já é
um indicativo de que o mesmo tem alguma relação com a consulta, que pode ser
forte ou fraca, em diversos graus. Porém, para a recuperação de informações,
além da presença, também é válido saber qual a frequência dos termos em um
documento. Intuitivamente, os documentos que referenciam os termos de uma
\emph{query} com mais frequência estão mais fortemente relacionados com a
mesma, e por isso deveriam receber uma pontuação maior. Então para cada termo
do documento é atribuído um peso proporcional ao número de ocorrências do mesmo,
designado como \emph{tf$_{t,d}$} (\emph{term frequency}). Em sua abordagem mais
simples, \emph{tf$_{t,d}$} é igual ao número de ocorrências de $t$ em $d$, mas
este valor pode ser normalizado, por exemplo, pelo tamanho do documento.

O conjunto de pesos determinado pelos \emph{tf}s dos termos de um documento
pode ser visto como um resumo quantitativo do documento. Essa visão do
documento é conhecida na literatura como ``saco de palavras'' (em inglês,
\emph{``bag of words''}), onde a disposição das palavras no documento é
ignorada e apenas a quantidade de ocorrências para cada termo é resguardada.

Por outro lado, alguns termos tem pouco ou nenhum poder de discriminação na
determinação de relevância de um documento e a medida \emph{tf} não considera
isto. Por exemplo, numa coleção de documentos sobre filmes, o termo
\emph{diretor} aparece em quase todos os documentos. O \emph{idf}$_t$
(\emph{inverse document frequency}) foi então introduzido para atenuar o efeito
destes termos no cálculo, diminuindo o peso de um termo por um fator que cresce
com sua frequência na coleção. Na equação \eqref{eq:idf}, $N$ representa o
número de documentos da coleção e $df_t$ (\emph{document frequency}) é o número
de documentos que contém o termo $t$.

\begin{equation}
\label{eq:idf}
idf_t = log \frac{N}{df_t}
\end{equation}

A medida \emph{tf-idf}$_{t,d}$ combina as definições de \emph{tf} e \emph{idf},
produzindo um peso composto (\emph{tf-idf}$_{t,d} = tf_{t,d} \times idf_t$) com
as seguintes propriedades \cite{Manning:09}:

\begin{enumerate}
  \item É alta quando ocorre muitas vezes em poucos documentos;
  \item Diminui quando ocorre menos vezes num documento ou em muitos documentos;
  \item É muito baixa quando o termo ocorre em quase todos os documentos.
\end{enumerate}

No cálculo de relevância geralmente são desconsiderados alguns termos,
designados como \emph{stop words}, que são muito frequentes e pouco
informativos do teor do documento. Artigos e pronomes, por exemplo, normalmente
figuram nesta categoria. A figura \ref{fig:exemplo_tfidf} apresenta um exemplo
de cálculo de pesos para um conjunto de documentos.

\begin{figure}[ht]
\centering
\includegraphics[width=.7\textwidth]{fig/recommendation.pdf}
\caption{Exemplo de cáculo de pesos para um conjunto de documentos}
\label{fig:exemplo_tfidf}
\end{figure}

Existem diversas variantes para o cálculo da medida \emph{tf-idf}$_{t,d}$,
propostas pouco a pouco pela comunidade de recuperação de informação com o
intuito de resolver algumas anomalias e melhorar os resultados de busca.
Por exemplo, é pouco provável que o aparecimento de uma palavra 20 vezes num
documento tenha de fato 20 vezes mais representatividade que uma ocorrência
única. Uma abordagem alternativa é incorporar o logaritmo no cálculo do
\emph{tf}. Outras modificações comuns são normalizações por diversas medidas:
pelo comprimento do documento, pelo comprimento médio dos documentos da
coleção, pelo \emph{tf} máximo ou médio entre os \emph{tf}s de todos os termos
do documento, entre outros. É comum também que elementos constantes sejam
introduzidos para suavizar um ou outro fator da fórmula.

Representando cada documento como um vetor de termos do dicionário e seus
respectivos pesos \emph{tf-idf} no documento, um cálculo possível de pontuação
de um documento para uma determinada \emph{query} seria a soma dos pesos dos
termos da consulta \eqref{eq:ranking}.

\begin{equation}
\label{eq:ranking}
R_{d,q} = \sum_{t \in q} tfidf_{t,d}
\end{equation}

\vspace{0.3cm} \hspace{-1.4cm}
\textbf{Similaridade de cossenos}

Medir a similaridade entre dois documentos pode ser útil, por exemplo, para
disponibilizar o recurso ``mais do mesmo'', onde o usuário pede indicações
de itens semelhantes a um que ele já conhece. Porém, se a diferença entre os
vetores de pesos de dois documentos for usada como medida para avaliação de
similaridade entre os mesmos, pode acontecer de documentos com conteúdo similar
serem considerados diferentes simplesmente porque um é muito maior que o outro.
Para compensar o efeito do comprimento dos documentos utiliza-se como medida a
similaridade de cossenos dos vetores que os representam, apresentada na equação
\eqref{eq:sim-cos}. O numerador representa o produto escalar dos dois vetores e
o denominador a distância euclidiana entre os mesmos.

\begin{equation}
\label{eq:sim-cos}
sim(d_1,d_2) = \frac{\vec{V}(d_1) \cdot \vec{V}(d_2)}{|\vec{V}(d_1)| |\vec{V}(d_2)|}
\end{equation}

Dado um documento $d$, para encontrar os documentos de uma coleção que mais se
assemelham mais a este, basta encontrar aqueles com maior similaridade de
cossenos com $d$. Para tanto, pode-se calcular os valores $sim(d,d_i)$ entre
$d$ e os demais $d_i$ documentos da coleção e os maiores valores indicarão os
documentos mais similares.

Uma coleção de $N$ documentos pode ser representada pelo conjunto de seus $N$
vetores, resultando numa visão da coleção como uma matriz de termos por
documentos. Tal matriz tem dimensões $M \times N$, onde as linhas representam
os $M$ termos da coleção. Esta representação conhecida como \emph{modelo de
espaço vetorial} e é amplamente utilizada em soluções para recuperação da
informação.

No modelo de espaço vetorial, não apenas documentos da coleção, mas também as
\emph{queries} podem ser representadas como vetores, se consideradas como
documentos pequenos. Portanto a similaridade de cossenos também pode ser
utilizada na busca por documentos relevantes para uma consulta, que serão os
que mais se assemelham à referida \emph{query}. Após o cálculo de similaridade
de cossenos entre os vetores da \emph{query} e dos documentos da coleção, os
maiores valores indicarão os documentos mais relevantes para consulta.

\subsubsection{Okapi BM25}

O modelo \emph{Okapi BM25} figurou por diversos anos como o melhor esquema de
pesos probabilísticos nas avaliações do TREC\footnote{O \emph{Text Retrieval
Conference (TREC)} é uma conferência anual realizada pelo \emph{U.S. National
Institute of Standards and Technology (NIST)} que promove uma ampla competição
em recuperação da informação de grandes coleções de texto com o intuito de
incentivar pesquisas na área.} e é considerado o estado da arte em recuperação
da informação \cite{Perez:09}. É uma evolução de outros dois esquemas,
\emph{BM11} e \emph{BM15} (\emph{BM} de \emph{"Best Match"}), sendo
fundamentado teoricamente na disciplina de probabilidade discreta, ao modelar
a frequência de termos em documentos utilizando distribuições de Poisson. Tem
esse nome porque o primeiro sistema no qual foi implementado se chamava
\emph{Okapi}.

O \emph{BM25} utiliza os componentes estatísticos do \emph{tf-idf}, porém numa
combinação diferente, além de considerar informações como o comprimento dos
documentos, a frequência dos termos na \emph{query} e outros fatores
constantes. A fórmula do \emph{BM25} é melhor compreendida através da análise
de seus três componentes principais, que neste texto são denominados de
$WT$, $WD$ e $WQ$.

$WT$ é a porção do peso que considera a frequência do termo no documento
($tf_{t,d}$), permitindo haver normalização pelo tamanho do documento. Na
fórmula \eqref{eq:WT}, $l_d$ representa o comprimento do documento $d$ e
$l_{avg}$ o comprimento médio dos documentos da coleção.

\begin{equation}
\label{eq:WT}
WT_{t,d} = \frac{tf_{t,d}(k_1+1)}{k_1((1-b)+b\frac{l_d}{l_{avg}})+tf_{t,d}}
\end{equation}

A constante $k_1$ determina o quanto o peso do documento deve ser afetado por
um acréscimo no valor de $tf$. Se $k_1=0$, este componente da formula é
eliminado, portanto $tf$ passa a não interferir no peso. Para valores altos de
$k_1$, o peso passa a ter um crescimento linear com relação a $tf$. De acordo
com experimentos do TREC, valores entre $1.2$ e $2$ são os mais indicados.
Valores dentro deste intervalo implicam numa interferência de $tf$ altamente
não linear, isto é, após 3 ou 4 ocorrências o impacto de uma ocorrência
adicional é mínimo \cite{Jones:00}.

O parâmetro $b \in [0,1]$ foi introduzido para ajustar o grau de normalização
pelo comprimento dos documentos. Se a configuração for $b=1$, a normalização
tem efeito completo (equivalente ao modelo \emph{BM11}). Valores menores
reduzem este efeito. Se $b=0$, o comprimento do documento não afeta a pontuação
final, (como no modelo \emph{BM15}).

O segundo componente do peso, $WD$, considera informações da atestação de
relevância provida pelo usuário (em inglês, \emph{relevance feedback}\footnote{
\emph{Relevance feedback} é o mecanismo através do qual o usuário avalia o
resultado da consulta, marcando os itens retornados como relevantes ou
irrelevantes, informação que é posteriormente utilizada para refinar a
consulta}), quando disponível. Na equação \eqref{eq:WD_rf}, $N$ é o número
total de documentos na coleção, $df_t$ é o número de documentos que contém o
termo $t$, $R$ é o número total de documentos relevantes e $r$ é o número de
documentos relevantes que contém o termo $t$.

\begin{equation}
\label{eq:WD_rf}
WD_{rf} = log \frac{(r+0.5)(N-df_t-R+r+0.5)}{(R-r+0.5)(df_t-r+0.5)}
\end{equation}

Se não houver informações provenientes da atestação de relevância, o $idf_t$
clássico pode ser utilizado \eqref{eq:idf}, ou ainda, uma variação de
$WD$ \eqref{eq:WD} a partir do estabelecimento de que $R=r=0$ na equação acima.

\begin{equation}
\label{eq:WD}
WD = log \frac{N-df_t+0.5}{df_t+0.5}
\end{equation}

A terceira parte da expressão, definida aqui como $WQ_{t,q}$ \eqref{eq:WQ}, permite
a consideração da frequência do termo na \emph{query}, que é apropriada para
o caso de consultas longas, podendo ser desconsiderada para \emph{queries}
com poucos termos.

\begin{equation}
\label{eq:WQ}
WQ_{t,q} = \frac{(k_3+1)qtf_{t,q}}{k_3+qtf_{t,q}}
\end{equation}

De forma análoga ao \emph{tf-idf}, o cálculo da pontuação do documento $d$ para
a \emph{query} $q$ no modelo \emph{BM25} pode ser realizado através da fórmula
\eqref{eq:bm25}.

\begin{equation}
\label{eq:bm25}
R_{d,q} = \sum_{t \in q} WT_{t,d} \times WD_t \times WQ_{t,q}
\end{equation}

O \emph{BM25} tem sido amplamente utilizado numa grande variedade tipos de
busca e coleções. Tem obtido sucesso especialmente nas avaliações do TREC,
o que provocou sua adoção por muitos grupos \cite{Jones:00}. A título de
exemplo, o Xapian\footnote{http://xapian.org/} e o
Lucene\footnote{http://lucene.apache.org/}, bibliotecas livres para construção
de ferramentas de busca, são projetos de grande destaque na comunidade que
utilizam o \emph{BM25} como medida de pesos \cite{Betts:07} \cite{Perez:09}.

\subsubsection{Classificador bayesiano}

Naive Bayes é uma solução para classificação que figura entre os algoritmos
de aprendizado de máquina \emph{supervisionados}. Cada exemplo utilizado
na fase de treinamento deve relacionar um objeto, caracterizado por um conjunto
de atributos, e uma classe. É dito supervisionado porque o ser humano que
atribuiu classes aos objetos dos exemplos atua como supervisor, como um
professor orientando o processo de aprendizado.

O classificador \emph{Naive Bayes} (Bayes ingênuo) tem como base um modelo
probabilístico que aplica o teorema de Bayes com fortes suposições de
independência de atributos -- por isso é considerado ingênuo. Em outras palavras,
a presença ou ausência de um atributo em uma classe não estaria relacionada com a
incidência de nenhum outro atributo. De certa forma a suposição de independência
é análoga no modelo de espaço vetorial, onde cada atributo é representado na
matriz por uma dimensão ortogonal a todas as outras \cite{Manning:09}.

A decisão acerca da classe a qual um objeto pertence é tomada de acordo com o
modelo de probabilidade máxima posterior (em inglês, \emph{maximum a posteriori
probability (MAP)}), indicada na equação \eqref{eq:map}. Dado que $C$ é o
conjunto de classes e $x$ objeto a ser classificado, a classe atribuída a este
será a que apresentar maior probabilidade condicionadada a $x$. $\hat{P}$ é
utilizado ao invés de $P$ porque geralmente não se sabe o valor exato das
probabilidades, elas são estimadas a partir dos dados de treinamento.

\begin{equation}
\label{eq:map}
c_{map} = \underset{c \in C}{\operatorname{arg\ max}} \hat{P}(c|x)
\end{equation}

A equação \eqref{eq:bayes} aplica o Teorema de Bayes para probabilidades
condicionadas. Na prática, apenas o numerador da fração interessa, visto que o
denominador é constante para todas as classes, portanto não afeta o $arg max$
\eqref{eq:bayes_no_deno}.

\begin{eqnarray}
\label{eq:bayes}
c_{map} & = & \underset{c \in C}{\operatorname{arg\ max}} \ \frac{\hat{P}(x|c) \hat{P}(c)}{\hat{P}(x)} \\
{} & {} & {} \nonumber \\
\label{eq:bayes_no_deno}
        & = & \underset{c \in C}{\operatorname{arg\ max}} \ \hat{P}(x|c) \hat{P}(c)
\end{eqnarray}

É neste ponto que a independência de atributos é importante. Considera-se que
um documento $x$ pode ser caracterizado por uma série de atributos $x_i$ -- no
caso de documentos de texto, os atributos são os próprios termos. Assumindo que
a ocorrência de atributos acontece independentemente, tem-se que:

\begin{equation}
\label{eq:independencia}
\hat{P}(x|c) = \hat{P}(x_1,x_2,...,x_n|c) = \hat{P}(x_1|c) \times \hat{P}(x_2|c) \times ... \times \hat{P}(x_n|c)
\end{equation}

Portanto, a função de decisão pode ser reescrita através da equação
\eqref{eq:bayes_prod}. Cada parâmetro condicional $\hat{P}(x_i|c)$ é um peso
que representa a qualidade do atributo $x_i$ como indicador da classe $c$,
enquanto que $\hat{P}(c)$.

\begin{equation}
\label{eq:bayes_prod}
c_{map} = \underset{c \in C}{\operatorname{arg\ max}} \ \ \hat{P}(c) \prod_{1 \le i \le n} \hat{P}(x_i|c)
\end{equation}

Os parâmetros são obtidos através da estimativa de maior probabilidade (em
inglês, \emph{maximum likelihood estimate (MLE)}), que é a frequência relativa
e corresponde ao valor mais provável de cada parâmetro, de acordo com os dados
de treinamento. A equação \eqref{eq:p_c} traz a estimativa de $\hat{P}(c)$,
onde $N_c$ é o número de objetos da classe $c$ e $N$ é o número total de
documentos.

\begin{equation}
\label{eq:p_c}
\hat{P}(c) = \frac{N_c}{N}
\end{equation}

As probabilidades condicionais são estimadas como a frequência relativa do
atributo $x$ em objetos que pertencem a classe $c$. Na equação \eqref{eq:p_xc},
$T_{cx}$ é o número de ocorrências de $x$ em objetos de exemplo da classe $c$ e
$V$ é o conjunto de atributos que os objetos podem apresentar.

\begin{equation}
\label{eq:p_xc}
\hat{P}(x|c) = \frac{T_{cx}}{\sum_{x' \in V} T_{cx'}}
\end{equation}

No entanto, a estimativa \emph{MLE} é zero para combinações atributo-classe que
não ocorrem nos dados de treinamento. Considerando que as probabilidades
condicionais de todos os atributos serão multiplicadas \eqref{eq:bayes_prod},
a simples ocorrência de uma probabilidade zerada resulta na desconsideração da
classe na referida classificação. E de fato, dados de treinamento nunca são
abrangentes o suficiente para representar a frequência de eventos raros de
forma adequada \cite{Manning:09}. Para eliminar zeros, adiciona-se $1$ a cada
termo da equação \eqref{eq:p_xc}:

\begin{equation}
\label{eq:p_xc+1}
\hat{P}(x|c) = \frac{T_{cx}+1}{\sum_{x' \in V} T_{cx'}+1}
\end{equation}

Apesar de a independência de atributos não ser verificada para a maioria
dos domínios de aplicação, na prática o Naive Bayes apresenta resultados
satisfatórios. \cite{Zhang:04} atribui a supreendente boa performance deste
método ao fato de que pode haver dependência forte entre dois atributos, desde
que as dependências sejam distribuídas igualmente nas classes. Segundo este
argumento, é a distribuição das dependências entre atributos ao através das
classes que afeta a classificação, não as dependências por si só.

\vspace{0.3cm} \hspace{-1.4cm}
\textbf{Variantes do modelo}

As duas principais variantes de implementação do classificador Naïve Bayes são
denominadas modelo \emph{multinomial} e modelo de \emph{Bernoulli} e diferem
fundamentalmente na maneira como os objetos são representados.

O primeiro modelo utiliza uma representação que de certa forma preserva
informações espaciais sobre o objeto. Na a classificação de documentos de
texto, por exemplo, o modelo gera um atributo que corresponde a um termo do
vocabulário para cada posição do documento. Já o modelo de \emph{Bernoulli}
produz um indicador para cada possível atributo (no caso de texto, termo do
vocabulário), onde $1$ representa a presença e $0$ a ausência do atributo no
objeto.

A escolha da representação de documentos adequada é uma decisão crítica no
projeto de um classificador. O próprio significado de um atributo depende da
representação. No \emph{multinomial}, um atributo pode assumir como valor
qualquer termo do vocabulário, o que resulta numa representação do documento
correspondente à sequência de termos do mesmo. Já para o modelo de
\emph{Bernoulli}, um atributo pode assumir apenas os valores $0$ e $1$, e a
representação do documento é uma sequência de $0$s e $1$s do tamanho do
vocabulário. A figura \ref{fig:exemplo_bayes} ilustra as peculiaridades de cada
representação.

\begin{figure}[ht]
\centering
\includegraphics[width=.7\textwidth]{fig/recommendation.pdf}
\caption{Exemplo ilustrativo dos modelos multinomial e de Bernoulli}
\label{fig:exemplo_bayes}
\end{figure}

\vspace{0.3cm} \hspace{-1.4cm}
\textbf{Seleção de atributos}

Uma grande quantidade de atributos a ser considerados resulta em alta
complexidade computacional, além de geralmente mascarar a presença de ruídos
nos dados, que prejudicam a acurácia da classificação. A fim de amenizar estes
problemas, é comum que seja feita uma seleção de atributos (em inglês,
\emph{feature selection}), que consiste em escolher alguns atributos que
ocorrem no conjunto de treinamento e utilizar apenas estes como parâmetro para
a classificação. Esta seleção equivale à substituição de um classificador
complexo por um mais simples. Especialmente quando a quantidade de dados de
treinamento é limitada, modelos mais fracos são preferíveis \cite{Manning:09}.

A seleção de atributos geralmente é realizada para cada clase em separado,
seguida pela combinação dos diversos conjuntos. Abaixo são apresentados alguns
métodos de escolha:

\begin{description}
  \item[Informação mútua.] Análise do quanto a presença ou ausência de um
    atributo contribui para a tomada de decisão correta por uma determinada
    classe. Informação mútua máxima significa que o atributo é um indicador
    perfeito para pertencimento a uma classe, que acontece quando um objeto
    apresenta o atributo se e somente se o objeto pertence à classe;
  \item[Independência de eventos.] Aplicação do teste estatístico $X^2$ para
    avaliar a independência de dois eventos -- neste caso, um atributo e uma
    classe. Se os dois eventos são dependentes, então a ocorrência do atributo
    torna a ocorrência da classe mais provável.
  \item[Baseado em frequência.] Seleção os atributos mais comuns para uma
    classe.
\end{description}

Ambos os métodos apresentados são ``gulosos'', ou seja, assumem escolhas ótimas
locais na esperança de serem ótimas globais. Como resultado, podem selecionar
atributos que não acrescentam nenhuma informação para a classificação quando
considerados outros previamente escolhidos. Apesar disto, algoritmos não
gulosos são raramente utilizados devido a seu custo computacional
\cite{Manning:09}.

\subsubsection{K-NN}

\emph{K-nearest neighbors} (k-NN), em português \emph{k vizinhos mais próximos}
é mais um algoritmo de aprendizado supervisionado para classificação. Esta
solução se baseia no conceito de vizinhança, que equivale a um conjunto de
objetos que apresentam características semelhantes.

O K-NN não exige nenhum treinamento prévio com os dados de exemplo, que podem
ser diretamente armazenados como vetores de atributos acompanhados por suas
devidas classes. A classificação de um novo objeto parte do cálculo da
vizinhança do mesmo, seguida pela atribuição da classe mais frequente entre os
seus $k$ vizinhos. $k$ é um parâmetro que define o tamanho da vizinhança.

O conceito de distância adotado deve ser o mais coerente para o domínio da
aplicação e conjunto de dados. Medidas comuns são a distância euclidiana,
distância de \emph{Hamming} e outras \emph{ad-hoc}. O k-NN também é sensível
a ruídos e sua performance geralmente é beneficiada pelo processo de seleção
de atributos.

% ver http://en.wikipedia.org/wiki/Neighbourhood_components_analysis
%pearson correlation p/ achar a vizinhanca - dependencia linear entre duas variaveis (em estatistica)
%seleção de atributos

%\subsubsection{Agrupamento}
%
%Agrupamento, \emph{clustering} em inglês, é uma técnica de aprendizado de
%máquina não supervisionado, ou seja, tenta identificar algum padrão de
%organização dos dados sem que haja uma classificação prévia dos exemplos.
%
%k-means - k sementes, agrupamento por similaridade, cálculo de novos centróides

\subsection{Estratégias de recomendação} \label{sec:estrategias}

Sistemas de recomendação são comumente classificados em três grandes grupos, de
acordo com a estratégia utilizada para o cálculo de recomendações: \emph{(1)}
baseada em conteúdo, \emph{(2)} colaborativa e \emph{(3)} híbrida. Neste
trabalho porém será apresentada a classificação proposta por \cite{Cazella:10},
que é um pouco mais detalhada.

\subsubsection{Reputação do item}

Bastante popular entre serviços de venda, como livrarias, sites de leilão e
lojas de modo geral, esta estratégia consiste no armazenamento de avaliações
dos produtos escritas por usuários e apresentação das mesmas no momento e
local apropriado \cite{Cazella:10}. A implementação desta solução é simples,
visto que exige apenas a manutenção dos dados originais, não sendo necessária
análise posterior alguma. No entanto, tem-se como premissa a imparcialiadade
dos usuários em suas opiniões, o que não pode ser verificado devido a seu
caráter subjetivo e estritamente pessoal. Atualmente existem serviços
especializados em reputação de produtos, que não têm venda associada, apenas
disponibilizam as avaliações. É o caso do \emph{Internet Movie Database}
\footnote{http://www.imdb.com/}, apresentado na figura \ref{fig:imdb}.

\begin{figure}[h]
\centering
\fbox{
  \includegraphics[width=1\textwidth]{fig/imdb.png}
}
\caption{Avaliação de usuário no IMDb}
\label{fig:imdb}
\end{figure}

\subsubsection{Baseada em conteúdo}

Este método parte do princípio de que os usuários tendem a se interessar por
itens semelhantes aos que eles já se interessaram no passado
\cite{Herlocker:2000}. O ponto chave desta estratégia é a classificação dos
itens, por exemplo, através da identificação de atributos (autores e temas de
livros, por exemplo). A partir dos atributos dos itens, aplica-se técnicas de
recuperação da informação para definir a semelhança entre os mesmos. No caso de
uma livraria, por exemplo, sugerir ao cliente outros livros do mesmo autor de
livros previamente selecionados é uma boa estratégia.

Pelo fato de se apoiar na classificação dos itens, os resultados da
recomendação são prejudicados nos casos em que os atributos não podem ser
identificados de forma automatizada. Outro problema indicado por
\cite{Adomavicius:05} é o da superespecialização, onde apenas itens similares
aos já escolhidos pelos usuários são recomendados.

\subsubsection{Colaborativa}

Esta abordagem não exige a compreensão ou reconhecimento do conteúdo dos itens,
pois se baseia na troca de experiências entre pessoas que possuem interesses
em comum.

Define-se uma função que representa a distância entre os usuários, e a
vizinhança de um usuário é composta pelos que estiverem mais próximos dele (com
menor distância). Uma recomendação para o usuário $u$ é produzida a partir
da análise dos itens da vizinhança de $u$. Os itens que aparecerem com maior
frequência na vizinhança farão parte da recomendação.

O problema da superespecialização é superado, visto que a recomendação neste
caso não se baseia no histórico do próprio usuário, podendo apresentar itens
totalmente inesperados. Outra contribuição é a possibilidade de formação de
comunidades de usuários pela identificação de seus gostos e interesses
similares \cite{Cazella:10}.

\subsubsection{Híbrida}

Consiste basicamente na combinação das abordagens colaborativa e baseada em
conteúdo, unindo o melhor das duas técnicas e eliminando as fraquezas de cada
uma \cite{Cazella:10}.

%\subsubsection{Filtragem baseada em outros contextos}
% (uso de perfis)

\subsubsection{Associação}

A partir da análise de uma base de dados que relaciona clientes e itens
selecionados pelos mesmos, utiliza-se diversas técnicas para inferir
associações entre os itens, como por e\-xemplo: "Clientes que selecionaram os
itens A, B e C também selecionaram o item D". As associações são caracterizadas
por um suporte mínimo, que diz respeito à confiabilidade das mesmas. Portanto,
quando uma associação é extraída, não significa que ela pode ser verificada em
todos os casos, e sim numa porcentagem mínima, definida pelo suporte. A
implementação desta estratégia é mais complexa, pois exige técnicas mais
sofisticadas para identificação de padrões no comportamento dos usuários,
principalmente quando se trata de um grande volume de dados. Um exemplo de
aplicação desta abordagem é encontrado no site da Amazon (figura
\ref{fig:amazon}).

\begin{figure}[h]
\centering
\fbox{
  \includegraphics[width=1\textwidth]{fig/amazon-association.png}
}
\caption{Recomendação por associação na Amazon}
\label{fig:amazon}
\end{figure}

\subsection{Avaliação de sistemas de recomendação} \label{sec:avaliacao}

A avaliação de sistemas de recomendação não é uma tarefa trivial,
principalmente porque não há consenso entre os pesquisadores sobre quais
atributos devem ser observados e quais métricas devem ser adotadas para
cada atributo \cite{Herlocker:2004}. Ademais, diferentes estratégias
podem funcionar melhor ou pior de acordo com o domínio da aplicação e as
propriedades dos dados. Por exemplo, algoritmos projetados especificamente
para conjuntos de dados com um número muito maior de usuários do que de itens
podem se mostrar inapropriados em domínios onde há muito mais itens do que
usuários.

\subsubsection{Compreensão das ações}

O processo de avaliação deve ter início com a compreensão das ações e
finalidades para as quais o sistema foi projetado. Abaixo estão relacionadas
algumas das ações identificadas por \cite{Herlocker:2004}.

\begin{description}

\item[Anotação em contexto]. Os primeiros sistemas de recomendação eram
  utilizados num cenário de informação estruturada (mensagens classificadas
  num contexto), e auxiliavam os usuários a decidirem quais mensagens valiam a
  pena serem lidas dentro de cada contexto.

\item[Encontrar itens relevantes]. O sistema sugere itens para o usuário
  através de uma lista produtos ordenados por uma pontuação proporcional
  à probabilidade do item ser considerado relevante pelo usuário. Esta é a
  finalidade mais comum relacionada a sistemas de recomendação, que atrai
  grande parte das pesquisas relacionadas com o tema e está presente na maioria
  dos sistemas de recomendação comerciais.

\item[Encontrar todos os itens relevantes]. Em situações onde não se deseja
  ignorar nenhum item relevante, ao invés da recomendação de alguns itens,
  todos os itens relevantes devem ser retornados.

\item[Sequência recomendada]. Quando não somente quais itens mas a ordem em que
  eles são apresentados importa, caracteriza-se a ação de recomendação de
  sequência.

\item[Expressão de opinião]. A recomendação em si muitas vezes não é o que
  atrai usuários desses sistemas. Alguns estão interessados simplesmente em
  emitir suas opiniões. Muito comum em ambientes que disponibilizam um espaço
  para os usuários registrarem suas avaliações sobre os produtos.

\item[Ajudar usuários]. Alguns usuários utilizam sistemas de recomendação
  por acreditarem que a comunidade se beneficia da sua contribuição. Apesar de
  nem sempre aparecerem juntas, tal atividade está comumente relacionada com a
  expressão de opinião.

\item[Navegação]. Recomendadores são normalmente avaliados de acordo com o
  quanto eles ajudam um usuário a tomar uma decisão de consumidor. Muitos
  usuários porém utilizam estes sistemas mesmo quando não têm planos de
  consumir nenhum produto, apenas para navegar através dos produtos. Neste
  caso, aspectos como a interface, facilidade de uso e natureza da informação
  provida são de extrema importância.

\end{description}

\subsubsection{Seleção dos dados}

Além da identificação das ações, outro fator importante para uma avaliação
consistente é a escolha do conjunto de dados adequado. Em certos casos a
avaliação pode ser realizada \emph{offline} em cima de uma base de dados
existente, já em outros, experimentos com usuários \emph{ao vivo} são mais
apropriados.

As análises \emph{offline} geralmente são objetivas, com foco na acurácia das
predições. Desde que existam dados prévios de avaliações dos usuários,
diferentes estratégias podem ser utilizadas para prever outros dados, e os
resultados são analisados utilizando-se uma ou mais métricas que serão
apresentadas nas próximas seções. Desta forma pode-se avaliar de maneira rápida
e pouco custosa diferentes conjuntos de dados e algoritmos. Em contrapartida,
tais análises são prejudicadas pela esparsidade dos dados. Não se pode, por
exemplo, avaliar a acurácia da recomendação de um item para um usuário se não
existe uma pontuação prévia do usuário para tal item.

Por outro lado, nos experimentos \emph{ao vivo}, seja num ambiente controlado
ou em campo, os sistemas são disponibilizados para uma comunidade de usuários
cujo comportamento é analisado no que diz respeito aos efeitos do sistema.
Neste tipo de experimento, além de análises objetivas como a acuácia, pode-se
avaliar fatores como a performance, participação e satisfação dos usuários.

Quando não existem dados prévios disponíveis ou quando existem porém não são
considerados adequados para o domínio ou a ação principal do sistema a ser
avaliado, pode-se ainda optar pelo uso de dados sintéticos. O uso de dados
artificiais é aceitável em fases preliminares de testes, porém, tecer
conclusões comparativas é arriscado, visto que os dados podem se ajustar
melhor para um algoritmo do que para outros \cite{Herlocker:2004}.

\subsection{Métricas}

[Detalhar cada item]

\begin{description}

\item[Acurácia de predição.] Medem o quanto pontuações previstas pelo
recomendador se aproximam das pontuações reais dos usuários. Exemplos: Erro
absoluto médio, erro quadrático médio, erro absoluto médio normalizado.

\item[Acurácia de classificação.] Medem a frequência com a qual o sistema faz
classificações corretas no que tange a relevância dos itens. Exemplos:
precisão, lembrança, medida F1, curva ROC, ...

\item[Além da acurácia.] Cobertura, curva de aprendizado, novidade e surpresa,
confiança, avaliação do usuário.

\end{description}

\section{Distribuições GNU/Linux} \label{sec:distribuicoes}

Em 1983 Richard Stallman criou o projeto GNU\footnote{\url{hhtp://www.gnu.org}}
com o objetivo de desenvolver um sistema operacional livre como alternativa ao
UNIX\footnote{\url{http://www.unix.org/}} -- solução comercial amplamente
difundida na indústria -- e que fosse compatível com os padrões
POSIX\footnote{\url{http://standards.ieee.org/develop/wg/POSIX.html}} --
família de normas definidas pelo IEEE com foco na portabilidade entre sistemas
operacionais. Nos anos 90 o projeto GNU já havia atraído muitos colaboradores,
que num curto espaço de tempo haviam desenvolvido inúmeros aplicativos para
compor o sistema operacional. Porém, o núcleo do sistema (\emph{GNU Hurd}) teve
desenvolvimento mais lento.

Em outubro de 1991 o estudante finlandês Linus Torvalds publicou a versão
0.02 do Freax, o núcleo de um sistema operacional (\emph{kernel}, em inglês)
desenvolvido por ele na universidade. Nem o próprio Linus imaginava que aquele
projeto, desenvolvido sem grandes pretensões, teria a dimensão do que hoje
conhecemos como Linux \cite{Linus:01}.

Com o anúncio de Torvalds, Stallman vislumbrou a possibilidade de agilizar o
lançamento do sistema operacional livre, se os aplicativos GNU que já estavam
prontos fossem combinados com o núcleo recém-lançado -- de fato, a primeira
versão estável do GNU Hurd foi lançada apenas em 2001. Em 1992 o Linux foi
licenciado sob a GNU GPL (\emph{General Public License\footnote{Suporte legal
para a distribuição livre de softwares}}) e as equipes dos dois projetos
começaram a trabalhar na adaptação do kernel Linux para o ambiente GNU. Este
esforço conjunto possibilitou o surgimento das distribuições GNU/Linux, que
são variações do sistema GNU/Linux.

Distribuições como Debian, Fedora, Mandriva e Ubuntu, oferecem diferentes
soluções de sistema operacional compostas pelo GNU/Linux e mais uma gama de
softwares selecionados pela comunidade ou empresa responsável por seu
desenvolvimento. As distribuições reduzem a complexidade de instalação e
atualização do sistema para usuários finais \cite{Cosmo:08}. Os mantenedores da
distribuição atuam como intermediários entre os usuários e os autores dos
softwares (chamados de \emph{upstreams}), através do encapsulamento de
componentes de software em abstrações chamadas \emph{pacotes}.

O processo de desenvolvimento e manutenção de uma distribuição varia bastante
de uma para outra e está diretamente ligado à constituição do projeto. Quando
são criadas por empresas, costumam receber colaboração dos usuários, mas de
forma limitada, visto que as decisões chave são tomadas dentro da organização.
Este é o modelo de desenvolvimento conhecido como catedral \cite{Raymond:99}.
Por outro lado, os projetos criados independentemente, formam ao longo do tempo
uma comunidade de desenvolvedores interessados em colaborar, e estes são os
únicos responsáveis pelo sucesso ou fracasso do projeto. Neste modelo, que
recebe o nome de bazar, o código-fonte está disponível durante todo o processo
de desenvolvimento, não apenas nos lançamentos, permitindo que a contribuição
seja mais efetiva. Nesses casos observa-se com mais clareza o fenômeno do
consumidor produtor descrito anteriormente. Segundo Raymond este modelo é
mais favorável ao sucesso, pois um bom trabalho de desenvolvimento de software
é motivado por uma necessidade pessoal do desenvolvedor (ou seja, o
desenvolvedor é também usuário).

A seleção e configuração dos aplicativos básicos de uma distribuição ficam
a cargo da equipe que a desenvolve, com diferentes níveis de interferência da
comunidade, como vimos acima. Este é um ponto chave no desenvolvimento,
pois é um dos fatores que mais influenciam a escolha dos usuários pela
distribuição. Um exemplo do impacto que tal seleção têm na comunidade foi a
recente polêmica gerada pelo anúncio de Mark Shuttleworth, criador do
Ubuntu, de que o projeto abandonaria o Gnome como interface padrão do usuário
\cite{Paul:10}. No caso do Debian, uma decisão como esta seria resultado de
longo período de discussão na lista debian-desktop. Polêmicas no Debian não são
raras, por exemplo, quando o assunto é o tema do desktop padrão (papel de parede,
cores e estilo das janelas, etc) que será lançado na próxima versão estável.

Softwares adicionais que atendem a necessidades específicas devem ser
instalados pelo usuário ou administrador do sistema, após a configuração do
sistema operacional. Este processo é facilitado pela infraestrutura de
instalação de softwares provida pela distribuição (baseada em pacotes), mas a
seleção dos programas fica a cargo do usuário.

\subsection{Contexto do trabalho}

A escolha da distribuição na qual o desenvolvimento do trabalho seria baseado
foi pautada pelos seguintes critérios: \emph{(1)} Existência de um esquema
consistente de distribuição de software; \emph{(2)} Existência de dados
estatísticos referentes ao uso de pacotes por usuários e possibilidade de
acesso aos mesmos; \emph{(3)} Possibilidade de integração dos resultados do
trabalho com os serviços dispobinilizados pela distribuição; \emph{(4)}
Popularidade da distribuição. O Debian GNU/Linux foi selecionado pelos fatores
apresentados a seguir.

\begin{enumerate}

\item O gerenciamento de pacotes em sistemas Debian GNU/Linux é realizado
através do \emph{APT (Advanced Packaging Tool)}. Ações como a busca, obtenção,
instalação, atualização e remoção de pacotes são disparadas pelo APT, que num
nível mais baixo faz uso do \emph{dpkg}. O APT também gerencia conflitos entre
pacotes, checando se novos pacotes a serem instalados conflitam com algum
outro do sistema, e dependências, instalando todos os programas definidos
como pré-requisitos antes da instalação de um pacote.

\item O \emph{Popcon (Popularity Contest)}\footnote{\url{http://popcon.debian.org}}
é um concurso de popularidade entre pacotes. Os usuários que aceitam participar
do concurso enviam periodicamente a sua lista de pacotes instalados no sistema,
que são armazenados no servidor do popcon. Diariamente as listas recebidas são
processadas e dados estatísticos acerca do uso dos pacotes são gerados e
disponibilizados no website do projeto.

\item Segundo o seu \emph{contrato social perante a comunidade de software
livre\footnote{\url{http://www.debian.org/social_contract.pt.html}}}, o
desenvolvimento do projeto Debian é guiado pelas necessidades dos usuários
e da comunidade de software livre. Portanto, as iniciativas de colaboradores
individuais, seja ele desenvolvedor oficial ou não, serão igualmente
consideradas e passarão a fazer parte da distribuição se seguirem aqueles
princípios e forem considerados de utilidade para a comunidade.

\item O Debian é um projeto de destaque no ecossistema do software livre. Desde
o lançamento da primeira versão de sua distribuição, em 1993, o projeto cresceu
bastante em termos de componentes de software (atualmente provê mais de 25.000
pacotes), colaboradores e usuários. A \emph{Distrowatch}, que tem 671
distribuições em sua base de dados, classifica o Debian GNU/Linux entre as 10
distribuições mais
populares\footnote{\url{http://distrowatch.com/dwres.php?resource=major}}.
O Debian aparece na quinta posição em suas estatísticas de páginas visitadas
\footnote{\url{http://distrowatch.com/stats.php?section=popularity}}. Já o
\emph{Linux Counter\footnote{\url{http://counter.li.org/reports/machines.php}}}
apresenta o Debian como a segunda distribuição mais popular entre as máquinas
cadastradas que rodam o kernel Linux ($16\%$), ficando atrás apenas do Ubuntu
($24\%$). Nas pesquisas da \emph{W$^{\textrm{3}}$Techs} sobre tecnologias para
serviços
web\footnote{\url{http://w3techs.com/technologies/history_details/os-linux}}, o
Debian aparece em segundo lugar, estando presente em $27\%$ dos servidores. Na
primeira posição está o CentOS com $31\%$.

\end{enumerate}

De maneira geral, quando o projeto Debian é mencionado trata-se não somente do
sistema operacional, mas de toda a infra-estrutura de desenvolvimento e
coordenação que dá suporte ao trabalho de cerca de 1500 desenvolvedores
oficiais, além de outros milhares de colaboradores ao redor do globo. O
trabalho é realizado de forma colaborativa, afinado pelo objetivo comum de
produzir e disponibilizar livremente um sistema operacional de qualidade para
seus usuários \cite{Jackson:98}. A interação entre os desenvolvedores acontece
majoritariamente através da Internet, por meio de canais IRC e listas de
discussão púlicas. Não existe uma entidade formal ou qualquer tipo de
organização que concentre, coordene ou defina as atividades do projeto. O que
observa-se é um modelo de governança consolidado que emergiu naturalmente ao
longo de sua história \cite{Ferraro:07}.

Diante do exposto, optou-se pelo Debian GNU/Linux como ambiente de
desenvolvimento, todavia, os resultados poderão facilmente ser adaptados para
outros contextos, desde que as informações necessárias para o cálculo de
recomendações estejam disponíveis (dados estatísticos).

\section{Recomendação nas distribuições} \label{sec:rec_distro}

Diante da complexa e crescente estrutura do projeto Debian, observa-se um
esforço por parte dos desenvolvedores, principalmente da equipe responsável
pelo controle de qualidade\footnote{\url{http://qa.debian.org}}, de reunir,
organizar e diponibilizar as informações ou meta-dados concernentes a esta
estrutura \cite{Nussbaum:10}. A tabela \ref{tab:recDebian} relaciona algumas
destas iniciativas que estão diretamente ligadas ao gerenciamento de pacotes.
Vale ressaltar que a maioria destas soluções foram inicialmente desenvolvidas
num contexto extra-oficial, mas a medida que se mostraram úteis e eficientes
foram absorvidas pela comunidade de usuários e desenvolvedores.

\begin{table}[h]
  \centering
  \caption{Soluções de recomendação no Debian}
  \label{tab:recDebian}
  \small
  \begin{tabularx}{\textwidth}{| c | X | c |}
    \hline
    \normalsize \emph{Solução} & \normalsize \emph{Descrição} & \normalsize \emph{Estratégia de recomendação}\\
    \hline BTS & Sistema de rastreamento de bugs, que é alimentado pelos
    usuários & \multirow{3}{*}{Reputação}\\
    \cline{1-2}
    Popcon & \emph{Popularity Contest} publica diariamente o resultado do
    concurso de popularidade entre pacotes & \\
    \cline{1-2}
    Packages.qa & Criado pelo time de qualidade, reune informações relativas
    à manutenção do pacote & \\
    \cline{1-2}
    UDD & \emph{Ultimate Debian Database} é outra iniciativa do time de
    qualidade, que reune informações de diversos aspectos do Debian numa base
    de dados única. Usuários avançados podem consultar esta base para tomar
    decisões acerca de que pacotes usar. & \\
    \hline
    Debtags & Classificação de pacotes em produção, porém,
    sem esquema de recomendação & Baseada em conteúdo\\
    \hline
    Debtags & No cadastro de tags para pacotes, novas tags são sugeridas a
    partir das tags já associadas ao pacote & Associação\\
    \hline
  \end{tabularx}
\end{table}

[Investigar iniciativas externas ao Debian: central de aplicativos ubuntu]

%[Pacotes Debian, Relação entre pacotes, O Repositório de Pacotes]

%\subsection{Pacotes Debian}
%. Binários e fontes\\
%. upstream, maintainer, uploader\\
%. Prioridade: required, important, standard, optional, extra\\
%. Base system = required or important (muitos são marcados como essenciais)\\
%. Essenciais: o gerenciador de pacotes se recusa a remover, a menos que seja forçado\\
%
%\subsection{Relação entre pacotes}
%. Dependência: Pre-depends, Depends, Recommends, Suggests, Enhances\\
%. Anti-dependência: Breaks, Conflicts, Replaces\\
%. Pacotes virtuais: Provides\\
%. Entre fontes e binários: Build-Depends, Build-Depends-Indep, Build-Conflicts, Build-Conflicts-Indep
%
%\subsection{O Repositório de Pacotes}
%. The Debian Archive,\\
%. Áreas: main, contrib, non-free\\
%. Seções\\

\section{Metodologia} \label{sec:metodologia}

O desenvolvimento deste trabalho será pautado pelos seguintes passos:
compreensão do problema; estabelecimento de estratégias para resolver o
problema; desenvolvimento; análise dos resultados. Nas seções seguintes são
apresenta

\subsection{Caracterização do problema}

Ao trazer o problema da recomendação para o contexto de distribuições GNU/Linux,
considera-se que os componentes de software ou pacotes são itens e os usuários
da distribuição são clientes. Em termos de entrada e saída do sistema de
recomendação: Dada a lista de pacotes de um determinado usuário (destinatário da
recomendação), deve-se retornar uma lista de pacotes recomendados, que
representam pacotes de potencial interesse para o usuário destinatário.

\begin{figure}[ht]
\centering
\includegraphics[width=.7\textwidth]{fig/recommendation.pdf}
\caption{Cenário de uma Recomendação}
\label{fig:recommendation}
\end{figure}

Outro ponto a ser ressaltado é que as recomendações devem ser geradas a partir
do comportamento do usuário (ação), e não da opinião. Neste trabalho não serão
consideradas informações registradas por usuários no BTS ou em listas de
discussão. Pretende-se calcular a recomendação a partir de dados do
\emph{Popcon}, que contém listas de pacotes instalados de milhares de sistemas
em produção, e do Debtags, como fonte de meta-dados sobre os pacotes.

Uma característica importante desta instância do problema de recomendação é
que, diferentemente da situação usual onde os itens não se relacionam entre si
(produtos na prateleira do supermercado, por exemplo), os componentes de
software objetos desta pesquisa (pacotes debian) podem declarar explicitamente
requisitos em seu conteúdo \cite{Cosmo:09}. Requisitos positivos representam
relações de dependência enquanto que os negativos representam conflitos, que
podem ser definidos em diversos graus: dependência, sugestão, recomendação,
conflito, substituição, quebra, etc. Por exemplo, se um componente $a$ depende
de $b$, significa dizer que $b$ deve ser estar instalado no sistema para que
$a$ funcione como previsto. Por outro lado, se $a$ conflita com $c$, a
instalação de ambos os programas pode provocar um comportamento anômalo ou até
comprometer o funcionamento de todo o sistema.

As relações entre os componentes são consideradas pelo sistema de gerenciamento
de pacotes (no Debian, o APT), que recebe do usuário um pedido de modificar
a instalação do sistema de alguma maneira -- por exemplo, a instalação de um
novo componente -- e tenta satisfazer o pedido a partir do conhecimento de onde
os componentes se encontram (repositórios de pacotes) e das relações entre os
componentes. O gerenciador então promove a instalação de todas as dependências
de um pacote antes de instalá-lo, e não permite a instalação de pacotes que
conflitem com outros já instalados no sistema.

Portanto, o cálculo da recomendação de pacotes deve levar em conta as
relações de dependência entre pacotes e desconsiderar pacotes dependentes
nas buscas por associações, visto que as dependências de um pacote sempre
(ou quase sempre) estão presentes nos sistemas com tal pacote instalado.

\subsection{Desenvolvimento de estratégias}

Para o cálculo da recomendação pretende-se experimentar as estratégias:
\begin{description}
  \item[Baseada em conteúdo] a partir da classificação provida pelo debtags;
  \item[Colaborativa] a partir dos dados do popcon;
  \item[Híbrida] com uma mesclagem das duas abordagens.
\end{description}

Na implementação de tais estratégias, pretende-se utilizar algumas das técnicas
mencionadas na seção \ref{sec:tecnicas} e fazer uma análise comparativa para
avaliar qual delas é mais viável para o sistema em produção.

\subsection{Desenvolvimento}

O desenvolvimento de software será majoritariamente realizado na linguagem de
programação Python e C, pela disponibilidade de bibliotecas de utilidade para
o trabalho.

\subsection{Avaliação}

De acordo com os pontos destacados na seção \ref{sec:avaliacao}, o plano de
avaliação do sistema de recomendação deve ser definido com base na
identificação da ação principal do sistema, que para este trabalho é
\emph{encontrar itens relevantes}.

Como já mencionado, a construção do recomendador será baseada nos dados do
\emph{Popcon} e \emph{Debtags}. Todavia os dados do \emph{Popcon} devem ser
pré-processados a fim de serem utilizados pelos algoritmos de recomendação.
As listas de pacotes enviadas pelos usuários contém todos os pacotes do
sistema, entre aplicativos, bibliotecas e até o kernel Linux. Para este estudo,
apenas os aplicativos para usuário final são relevantes, pois são eles que
devem compor a recomendação. Os pacotes que fazem parte do sistema básico
também devem ser desconsiderados, pois em tese todos os usuários já os possuem
visto que são essenciais para o funcionamento de qualquer sistema Debian.
Na literatura este pré-processamento é denominado de \emph{seleção de
atributos}, que é realizado com o intuito de diminuir o ruído nos dados e
reduzir sua ordem de grandeza.

Diante disponibilidade limitada de dados, que inviabiliza a medição
\emph{offline} da acurácia de recomendação, optou-se por realizar experimentos
\emph{ao vivo} com usuários, por meio de um \emph{survey} eletrônico.
Pretende-se disponibilizar um site onde o usuário seja guiado nos seguintes
passos:
\begin{enumerate}
  \item O usuário envia a sua lista de pacotes instalados, como representação
    de identidade;
  \item O sistema realiza a computação necessária para gerar recomendações
    utilizando diferentes estratégias;
  \item As recomendações são apresentadas ao usuário, juntamente com informações
    detalhadas de cada item, além de explicações acerca do processo realizado;
  \item O usuário avalia as recomendações apresentadas;
  \item Com base na avaliação do usuário, aplicar as seguintes métricas: erro
    absoluto médio, precisão, lembrança, novidade e surpresa e confiança.
\end{enumerate}

Ao final da etapa de avaliação, os dados de avaliação submetidos por todos os
usuários serão compilados em gráficos e comentários apresentados na versão
final da dissertação deste trabalho.

Ver: http://www.limesurvey.org/

\subsection{Plano de execução}

O desenvolvimento deste trabalho está previsto para acontecer entre os meses
de janeiro e julho de 2011. Considerando o macro objetivo de desenvolver um
sistema de recomendação de pacotes, metas intermediárias foram estabelecidas
e apresentadas na tabela \ref{tab:plano}.

\begin{table}[h]
  \centering
  \caption{Plano de execução}
  \label{tab:plano}
  \small
  \begin{tabularx}{\textwidth}{| X | c | c | c | c | c | c | c |}
    \hline
    \normalsize \emph{Atividade} & \normalsize \emph{Janeiro} & \normalsize \emph{Fevereiro} & \normalsize \emph{Março} & \normalsize \emph{Abril} & \normalsize \emph{Maio} & \normalsize \emph{Junho} & \normalsize \emph{Julho}\\
    \hline
    Preparação e realização da qualificação & X & X & & & & & \\
    \hline
    Implementação de diferentes estratégias e técnicas & X & X & X & X & & & \\
    \hline
    Testes e ajustes na implementação & & & & X & X & & \\
    \hline
    Preparação e aplicação do \emph{survey} & & & & & X & X & \\
    \hline
    Escrita da dissertação & & & & & X & X & X \\
    \hline
  \end{tabularx}
\end{table}

\section{Conclusão} \label{sec:conclusao}

A conclusão do trabalho será pautada pela análise dos resultados obtidos com
a aplicação do \emph{survey}, seguida por uma proposta de integração do sistema
de recomendação com a infraestrutura do projeto Debian.

\bibliographystyle{sbc}
\bibliography{quali-tassia}

\end{document}
